# Design notes in no particular order
We could have a C program that did OOP with a vtable pointer at the start of
each object. That vtable could easily contain primitive operations for tracing
GC, reporting the object size, moving itself within memory, etc. That vtable
could also itself be a parser.

We need this to be concatenative. An integer is "evaluated" by pushing itself
onto the stack, which is, conveniently, a fixed point.

phi is basically a FORTH based on parsers rather than one-layer concatenative
design. This demands some complexity from the interpreter because we need to be
able to use parsers as incremental reducers over inputs, much like the FORTH
interpreter consumes input as it goes. I think this means the return stack is
actually comprised of parser-continuations that conditionally add things.

...and this means the REPL is really a value whose parse continuation
side-effectfully reads input and generates values.

**TODO:** lay out the concatenative evaluation model, particularly the way
values-as-parsers interact with continuations. Run through a quick example.

- OK here's the problem. If we're reading source, presumably from some IO
  channel, the code drives the channel (in FORTH). In phi, the code might drive
  some kind of buffer and destructively consume from time to time.
    - What's the equivalent of a readable IO channel for continuations? WHOA
      hang on here - is it a canard-style nested list of returns? Is this
      generated by seq parsers? (Then Q: how do `alt` parsers work?)

- Canard-style return lists assume a monomorphic executor; that would be just
  `seq` parsers with no `alt`.
- phi-style polymorphic return lists require something different; now we have
  polymorphism that the interpreter interacts with. So we don't just have cons
  cells; now we have `seq` conses and `alt` conses, more or less.
    - success and fail are _messages_, not just data elements. `seq` and `alt`
      respond to these.
    - `success(pk, v)` and `fail(pk)` each describe the continuation from that
      point
    - what does it mean to "respond to" a message?
- the "resolver", in canard terms, is a thing that rewrites the data stack.
    - if parsing source lexically, we implicitly `alt` into the stack-top value
    - let's think this through a bit

- OK suppose we have a "stack evaluator" object that maintains state + contains
  code. There's one that provides FORTH-style execution; maybe that's the
  default. But you can create a new one to push quoting down by a level. We
  always catalyze the data stack using parsers?
    - If the data stack is auto-catalyzing, how do we refer to a function as an
      object? Maybe have a "function-as-object" object type, or have an "apply
      function" action. Functions as objects are inert until applied.

**Q:** how do we manage memory for parse states, given that failover is
theoretically always possible? Maybe buffering is done by `alt` elements, which
tail-call into their final option. Use GC to free stuff, so this is all just
normal reference tracking.

**Q:** should the stack-top value assume control over the stack parse? Sure;
that's how we do polymorphism.

**Q:** if we're living in a world of alternatives/etc, how do we encode nested
parsers? Do we have nonlocal returns? (Arguably no, since `alt` has bounded
failure.)

## Example evaluation equations
```
i alt(a, b) .parse = i a .parse
                   | i b .parse

i seq(f, a, b) .parse =
  let (v1, i2) = i  a .parse in
  let (v2, i3) = i2 b .parse in
  v1 v2 f .apply i3
```

**Q:** how do we know to polymorphic-apply the second stack value to a method
name? Is this something the boot stack parser does automatically? (I don't see a
problem with this at the moment.)

The above equations sort-of describe how continuations work, but it's not all
there; both `alt` and `seq` leave some stuff on the continuation stack.

## Polymorphic continuation parsing
Ok, suppose a _value_ can ask for the execution continuation -- which of course
is determined. It can then overload the parser used to execute that
continuation.

**Q:** do we still have local scopes that encode behavior for things? I don't
see why not; they track parse-time constructs and are (I think) mostly erased by
runtime.

The FORTH-style equivalent of inlining is list-appending; if we have some word
bound to a concatenative definition, a parser looking for continuation stuff
would then match into the inlined list if it wanted to. (**Q:** does this commit
us to backtracking?)

If this is true, then we could have a native "just run the word" parser as the
last element of the alt that governs value-contination overloading.

Put differently, phi is FORTH with polymorphism, possibly without a data stack.

Hang on: FORTH used a compile/immediate flag, but suppose we control the "active
scope" -- it would achieve much more with a similar amount of simplicity.

### Core execution model (proposal)
If we've got conses and such, let's build stack primitives as functions that
manipulate a "stack top" binding in various ways. Then we can implement parsers
and other functions concatenatively: compilation involves reducing a
multichannel program down to a single concatenation.

This is missing the point; if scopes are still driving polymorphism, then a
global stack serves no purpose because scopes already provide extensible (and
efficient) addressing.

### What if every value _is_ a parser?
Parsers consume concatenative. So we have some elementary value/parser things,
and we have baseline rules that perform higher-order parser operations to those.
This might sorta work.
