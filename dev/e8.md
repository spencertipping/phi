# `e8` vtables
1. Values are callable as AMD64 machine code
2. The initial instruction in any given value is probably `e8` to call the
   vtable, with the return address set to the instance data
3. Control flow is driven by objects
4. Garbage collection is also driven by objects (i.e. it's a hosted process)


## Method calls
Methods are numeric, assigned by protocol objects. They should refer into
indexed slots in vtables, and the method to be invoked should be stored in a
register, e.g. `%rax`.

**NB:** if objects are pointed to directly, we can't do complex object
addressing, e.g. records within columnar data structures. The records need to at
least provide the `e8 disp32` five-byte prefix and be addressible.

We may be able to get around this using pointer tagging to indicate that some
dereferencing is required. It also may just be a lost cause: suppose we've got
tagged pointers that refer to single bytes within some data structure; how do we
get from there to the hosting object without doing some kind of memory
alignment (which would be terrible)? Let's table this for now.

Back to method calls ... here's what I'm thinking (**NB:** the below is now a
lie):

```
              return address from e8 (on the return stack)
              |
              V
  e8 <vtable> instance-data ....
      |
      V
      48b9 <methodlist-address>         # %rcx = &methodlist
      488b o014o310                     # %rcx = *(%rcx + 8*%rax)
      ff o341                           # jmp *%rcx
```

Why encode the `e8` byte instead of just prefixing the object with the vtable
address? **No reason:** we can get the same type of polymorphism by having an
object store its own vtable and turning the `e8` into a nop: `e8 00000000`. Then
`e8` is implied and our overhead is down to 4 bytes, likely expanded to the
absolute 8 to keep everything simple. (Or we can limit the heap to 1GB.)


## Stack protocol
The current interpreter should be stored in a register, let's say `%rbp`, and it
should specify the heap object into which new values are allocated. Method calls
to the heap can use the system stack and registers, but end up being inlined
from the interpreter's point of view -- that is, we don't have intermediate cons
cell states.

This implies that method calls in general can happen without committing anything
into the interpreter's continuation stack. That's true of the above strategy,
for instance. However, most method calls _should_ move return addresses from the
system stack to the continuation stack for two reasons:

1. It should be cheap to quote the interpreter (e.g. for `call/cc` or whatever)
2. We can then use the system stack for GC tracing without worrying about
   overflow

There's no free lunch, though: incrementally migrating the system stack to a
cons structure is no faster than catching up if we hit an `i>` instruction. So
the obvious move is still to stack-allocate return addresses. Maybe the
interpreter is responsible for maintaining the `%rsp` memory as some type of
buffer (and detecting overflow-via-segfault?); then at least it's encapsulated.

...so the interpreter is mutable until we quote it, shifting the overhead to
`i>`. Don't use `i>` in loops because one way or another it will make the entire
world slower.


## Stepping the interpreter
The interpreter needs to provide a chunk of machine code that can be run to step
it, presumably with the interpreter itself in `%rbp`. Then we tail-jump into a
looped version of that and never look back. So we probably have some
minimalistic JIT to produce the run loop.

Once we do this, the runtime is driven by the interpreter itself. There's no
implication that the interpreter would be polymorphic (despite being an object,
most likely), nor that you could interface with it in any other way. The latter
point is important because it means the heap can relocate the interpreter; the
only permgen-like thing we have is this tiny chunk of code, and even that could
be heap-allocated and relocated in theory. Groovy.

...so given all of this, the interpreter is entirely free to manage its stacks
as mutable push/pop things or as cons structures; all it needs to do is specify
how to trace and rewrite them for GC. The stacks don't need to be stored inside
the heap, either, because the interpreter is the GC root object. (This matters
because it means the stacks won't need to be copied by a mark-sweep collector,
which reduces GC overhead.)


## Garbage collection
I think we can do this with a single method, `mark(heap)`, which does three
things:

1. Allocates space for the current object in the specified heap
2. Marks the current object as having been GC-visited (i.e. its vtable is
   modified to a "moved object" object, and we store the new address)
3. Calls `mark()` on all objects we point to

It's important to do (2) before (3) so we don't fail on circular references, so
we need to buffer the first eight bytes of data from the value being marked.

GC can happen at any point, which means the stack may be arbitrarily deep; we
need to make sure things like long lists don't overflow it. We know that the
stack depth is limited by the live set size, but we don't have much beyond that.
I suppose we can reserve 3x the heap size: 1x for the old heap, 1x for the new
heap, and 1x for the GC stack (which is distinct from the regular stack).

### GC stack allocation
An object is at least 16 bytes because we need to be able to rewrite any
allocated thing into an "I've been marked" proxy object during GC. So let's do
some math. The worst case is that we have no garbage and that every object gets
copied into the new heap verbatim. If each method call nets eight bytes of
`%rsp` space while it's happening, then we should be able to allocate a fixed
amount of extra `%rsp` headroom and share the stack and the heap, if we assume
that `mark(heap)` uses tail calls.

### Weak references
Not entirely straightforward because we do need to update a weak pointer, but we
can't currently ask the referent for its new address without reserving space for
it in the new heap. I think we want a second heap scan to update weak pointers
within objects, which means heaps need to be scannable. This should be fine
since allocations are sequential and objects know their physical size. We just
need a new method `.physical_size()` for it.

**NB:** if we do this, we need the GC `mark` operation to be non-destructive.
This means we'd need to have the JVM's 16-byte allocation overhead unless we can
do something clever. Womp womp.

### Finalizers
We don't currently have finalizer support because we copy the live set rather
than deallocating objects individually. But heap enumeration would solve this
problem for us.


## Generalized JIT
The interpreter and `eval()` all contribute to overhead that we don't
necessarily want. If we assume that the world consists of mutable
data/continuation stacks, like FORTH, then our goal is to eliminate OOP overhead
from evaluation as much as we can.

For perspective, OOP overhead is pretty minimal per call; there's just a lot of
it. I think the current method invocation strategy looks like this:

```
# put the object into %rcx, method into %rax
call *%rcx                              # invoke the dispatch code
  push %rcx                             # push object so we have it
  movq $..., %rcx                       # load method table
  movl %eax, %edx                       # isolate method selector in low 32 bits
  jmp *(%rcx + 8*%rdx)                  # tail-call into method implementation
    pop %rcx                            # get the object back
    # shrq %rax, 32 if we care about the method index
```

...so all told, and assuming no cache misses, we've got something like six or
eight cycles per method call. We can optimize this by disabling `%rax`
index-packing and by removing vtable polymorphism; then the caller is
responsible for the indexed `call` instruction:

```
# object in %rcx, method index (by itself) in %rax
movq *%rcx, %rdx                        # %rdx = object->vtable
call *(%rdx + 8*%rax)                   # call selected method
```

Two cycles per call; no need to `pop %rcx` anymore because we're saving that
register. I think this strategy is actually faster than standard C calling
convention -- we just make a lot more calls into tiny functions. Optimization
still amounts to inlining, but obviously we don't need to inline much; any
method longer than a few instructions drops the OOP overhead below 50%.

Using OOP method calls helps enormously with JIT: we can have native code
snippets whose `eval()` method points to the code, and whose `compile(buffer)`
method emits that code into the buffer. From a data perspective, here's what one
of those objects might look like:

```
0:  vtable -> [ 0:  .eval() -> jmp *(%rcx + 12),
                8:  .compile(buffer) -> ...,
                16: .mark(heap) -> ...,
                ... ]
8:  int32 code-length
12: code...
```

...so compilation is exactly the same thing as inlining.

### Calling convention, formalized
Let's make some register assignments:

- `%rcx` (register 1): the receiver
- `%rax` (register 0): the method (should this and `%rcx` be swapped?)
- `%rsp` (register 4): the return stack pointer
- `%rbp` (register 5): the interpreter
- `%rsi` (register 6): the data stack pointer (data stack grows downwards)
- `%rdi` (register 7): the heap pointer (heap grows upwards)

Callees can clobber everything except `%rbp`, `%rsi`, `%rdi`, and `%rsp`.

`%rsi` is the data stack because the `lodsq` instruction lets us quickly pop
entries into registers.

Technically, keeping stack pointers in registers is redundant given that we
store the interpreter object. There are a few reasons it makes sense though:

1. For performance
2. To accommodate instructions like `call` and `ret`
3. To simplify the calling convention, which needs to be monomorphic anyway

Time to write this puppy.

### vtable relocation
vtables store absolute code addresses, which is a problem because GC will move
them. Two options:

1. Allocate vtables into a permgen that doesn't move (lame)
2. Have vtables refer into the code-field of each of a series of objects, and
   have the `mark` function decrement the addresses by the header size

Let's go with (2) because it's more fun. Here's what this looks like:

```
    vtable instance-data...                 # <- an object
         |
         V
vtable n codeptr1 codeptr2 codeptr3 ...
                |        |        |
                V        |        ...
     vtable len code...  |
                         V
              vtable len code...
```

We don't hard-code the 12-byte vtable/codeptr offset into the calling convention
because that adds another byte the machine code of every method call.

This way everything is an object (groovy), the GC manages relocation for us, and
we still get static-compilation performance because our pointers are
pre-resolved for the common eval-case.

**NB:** functions-as-objects don't give us polymorphism here; e.g. we can't use
this strategy to implement lexical closures as objects _unless_ the code offset
is exactly the same.

### vtable marking during GC
Do we want to live dangerously?

...yeah, we want to live dangerously. OK, here's how this works. Objects that
are marked have some memory overwritten, but it isn't a ton of memory. So we can
define a new marked-vtable class that overwrites memory as usual, but make sure
that the things it overwrites don't have anything to do with GC. So, for
instance, if the `mark` method is at vtable index 2 (byte 16), then we'll store
the relocated address in vtable index 0, which won't be called during GC. All of
our GC-triggered method calls continue to work correctly from a monomorphic call
structure even though our vtable is technically invalid.

(If we wanted to live slightly less-dangerously, we could burn 8 more bytes
_before_ the vtable pointer for the relocation address. But where's the fun in
that?)
