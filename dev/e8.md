# `e8` vtables
1. Values are callable as AMD64 machine code
2. The initial instruction in any given value is probably `e8` to call the
   vtable, with the return address set to the instance data
3. Control flow is driven by objects
4. Garbage collection is also driven by objects (i.e. it's a hosted process)

## Method calls
Methods are numeric, assigned by protocol objects. They should refer into
indexed slots in vtables, and the method to be invoked should be stored in a
register, e.g. `%rax`.

**NB:** if objects are pointed to directly, we can't do complex object
addressing, e.g. records within columnar data structures. The records need to at
least provide the `e8 disp32` five-byte prefix and be addressible.

We may be able to get around this using pointer tagging to indicate that some
dereferencing is required. It also may just be a lost cause: suppose we've got
tagged pointers that refer to single bytes within some data structure; how do we
get from there to the hosting object without doing some kind of memory
alignment (which would be terrible)? Let's table this for now.

Back to method calls ... here's what I'm thinking:

```
              return address from e8 (on the return stack)
              |
              V
  e8 <vtable> instance-data ....
      |
      V
      48b9 <methodlist-address>         # %rcx = &methodlist
      488b o014o310                     # %rcx = *(%rcx + 8*%rax)
      ff o341                           # jmp *%rcx
```

Why encode the `e8` byte instead of just prefixing the object with the vtable
address? **No reason:** we can get the same type of polymorphism by having an
object store its own vtable and turning the `e8` into a nop: `e8 00000000`. Then
`e8` is implied and our overhead is down to 4 bytes, likely expanded to the
absolute 8 to keep everything simple. (Or we can limit the heap to 1GB.)

## Stack protocol
The current interpreter should be stored in a register, let's say `%rbp`, and it
should specify the heap object into which new values are allocated. Method calls
to the heap can use the system stack and registers, but end up being inlined
from the interpreter's point of view -- that is, we don't have intermediate cons
cell states.

This implies that method calls in general can happen without committing anything
into the interpreter's continuation stack. That's true of the above strategy,
for instance. However, most method calls _should_ move return addresses from the
system stack to the continuation stack for two reasons:

1. It should be cheap to quote the interpreter (e.g. for `call/cc` or whatever)
2. We can then use the system stack for GC tracing without worrying about
   overflow

There's no free lunch, though: incrementally migrating the system stack to a
cons structure is no faster than catching up if we hit an `i>` instruction. So
the obvious move is still to stack-allocate return addresses. Maybe the
interpreter is responsible for maintaining the `%rsp` memory as some type of
buffer (and detecting overflow-via-segfault?); then at least it's encapsulated.

...so the interpreter is mutable until we quote it, shifting the overhead to
`i>`. Don't use `i>` in loops because one way or another it will make the entire
world slower.

## Stepping the interpreter
The interpreter needs to provide a chunk of machine code that can be run to step
it, presumably with the interpreter itself in `%rbp`. Then we tail-jump into a
looped version of that and never look back. So we probably have some
minimalistic JIT to produce the run loop.

Once we do this, the runtime is driven by the interpreter itself. There's no
implication that the interpreter would be polymorphic (despite being an object,
most likely), nor that you could interface with it in any other way. The latter
point is important because it means the heap can relocate the interpreter; the
only permgen-like thing we have is this tiny chunk of code, and even that could
be heap-allocated and relocated in theory. Groovy.

...so given all of this, the interpreter is entirely free to manage its stacks
as mutable push/pop things or as cons structures; all it needs to do is specify
how to trace and rewrite them for GC. The stacks don't need to be stored inside
the heap, either, because the interpreter is the GC root object. (This matters
because it means the stacks won't need to be copied by a mark-sweep collector,
which reduces GC overhead.)

## Garbage collection
I think we can do this with a single method, `mark(heap)`, which does three
things:

1. Allocates space for the current object in the specified heap
2. Marks the current object as having been GC-visited (i.e. its vtable is
   modified to a "moved object" object, and we store the new address)
3. Calls `mark()` on all objects we point to

It's important to do (2) before (3) so we don't fail on circular references, so
we need to buffer the first eight bytes of data from the value being marked.

GC can happen at any point, which means the stack may be arbitrarily deep; we
need to make sure things like long lists don't overflow it. We know that the
stack depth is limited by the live set size, but we don't have much beyond that.
I suppose we can reserve 3x the heap size: 1x for the old heap, 1x for the new
heap, and 1x for the GC stack (which is distinct from the regular stack).

### GC stack allocation
An object is at least 16 bytes because we need to be able to rewrite any
allocated thing into an "I've been marked" proxy object during GC. So let's do
some math. The worst case is that we have no garbage and that every object gets
copied into the new heap verbatim. If each method call nets eight bytes of
`%rsp` space while it's happening, then we should be able to allocate a fixed
amount of extra `%rsp` headroom and share the stack and the heap, if we assume
that `mark(heap)` uses tail calls.

## Generalized JIT
The interpreter and `eval()` all contribute to overhead that we don't
necessarily want. If we assume that the world consists of mutable
data/continuation stacks, like FORTH, then our goal is to eliminate OOP overhead
from evaluation as much as we can.

For perspective, OOP overhead is pretty minimal per call; there's just a lot of
it. I think the current method invocation strategy looks like this:

```
# put the object into %rcx, method into %rax
call *%rcx                              # invoke the dispatch code
  push %rcx                             # push object so we have it
  movq $..., %rcx                       # load method table
  movq %eax, %edx                       # isolate method selector in low 32 bits
  jmp *(%rcx + 8*%rdx)                  # tail-call into method implementation
    pop %rcx                            # get the object back
    # shrq %eax, 32 if we care about the method index
```

...so all told, and assuming no cache misses, we've got something like six or
eight cycles per method call. We can optimize this by disabling `%rax`
index-packing and by removing vtable polymorphism; then the caller is
responsible for the indexed `call` instruction:

```
# object in %rcx, method index (by itself) in %rax
movq *%rcx, %rdx                        # %rdx = object->vtable
call *(%rdx + 8*%rax)                   # call selected method
```

No need to `pop %rcx` anymore because we're saving that register. I think this
strategy is actually faster than standard C calling convention -- we just make a
lot more calls into tiny functions.
